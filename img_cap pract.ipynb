{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forbidden-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image  import load_img,img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input,decode_predictions,VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "olive-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "informal-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=load_img('cheese.jpg.jfif',target_size=(224,224) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "portuguese-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "viral-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "satellite-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=preprocess_input(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "southern-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suburban-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "label=decode_predictions(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afraid-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=label[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "innovative-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "butternut_squash(9.18%)\n"
     ]
    }
   ],
   "source": [
    "print('%s(%.2f%%)'%(label[1],label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abstract-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\parth\\.conda\\envs\\gputes\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\parth\\.conda\\envs\\gputes\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "brief-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4854917717073234\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'sleepy', 'dog']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "underlying-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      ">1000268201_693b08cb0e.jpg\n",
      "Extracted Features: 1\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os import path\n",
    "from pickle import dump\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    \n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    \n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "    # summarize\n",
    "    model.summary()\n",
    "    \n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        \n",
    "        # load an image from file\n",
    "        filename = path.join(directory, name)\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        \n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        \n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        \n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        \n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        print('>%s' % name)\n",
    "        return features\n",
    "# extract features from all images\n",
    "directory = 'E:\\Work\\Work_books\\python-project-image-caption-generator\\img_caption\\captions\\Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "\n",
    "print('Extracted Features: %d' % len(features))\n",
    "\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "interstate-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "irish-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "posted-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    for _, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # remove punctuation from each token\n",
    "            desc = [re_punc.sub('', w) for w in desc]\n",
    "            # remove hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] = ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "naval-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sonic-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "filename = 'E:\\Work\\Work_books\\python-project-image-caption-generator\\img_caption\\captions\\Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    \n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "# load document\n",
    "doc = load_doc(filename)\n",
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "# split line by white space\n",
    "tokens = line.split()\n",
    "26.7. Generate New Captions 325\n",
    "# split id from description\n",
    "image_id, image_desc = tokens[0], tokens[1:]\n",
    "# skip images not in the set\n",
    "if image_id in dataset:\n",
    "# create list\n",
    "if image_id not in descriptions:\n",
    "descriptions[image_id] = list()\n",
    "# wrap description in tokens\n",
    "desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "# store\n",
    "descriptions[image_id].append(desc)\n",
    "return descriptions\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "all_desc = list()\n",
    "for key in descriptions.keys():\n",
    "[all_desc.append(d) for d in descriptions[key]]\n",
    "return all_desc\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "lines = to_lines(descriptions)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "return tokenizer\n",
    "# load training dataset\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-right",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-participation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
